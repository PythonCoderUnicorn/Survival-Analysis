

# Fitting Regression Models

in the COPD dataset

walking ability has variables `MWT1, MWT2, MWT1Best` which means 6 minute walking test, distance measured in meters in 6 minute duration

look at the data, understand it, what types are the variables?

```{r}
library(tidyverse)

COPD = read_csv('./COURSERA-copd.csv')

glimpse(COPD)
```

look at the data

```{r}
hist(COPD$MWT1Best, breaks = 12)
```

the distribution looks symmetric, peaks between 400 and 500, have an outlier past 650


`subset(COPD, COPD$MWT1Best > 650)` is the same as filter()

```{r}
COPD %>% 
  filter(MWT1Best > 650)
```


look at another variable

```{r}
COPD %>% 
  ggplot( aes(x= FEV1))+
  geom_histogram(bins = 9)
```


Descriptive statistics and two-way plots can tell you more about the data. Useful statistics to summarise your data are: the mean, the standard deviation, the range, the median, and the inter-quartile range.

```{r}

skimr::skim(COPD$MWT1Best)
```


```{r}
skimr::skim(COPD$FEV1)
```


```{r}
COPD %>% 
  select(FEV1, MWT1Best) %>% 
  ggplot(
    aes(x= FEV1, y= MWT1Best)
  )+
  geom_point()+
  labs(x='FEV1', y='MWT1Best')+
  ggdark::dark_theme_gray()
```
the scatterplot shows an even distribution, no outliers, so proceed with Pearson's correlation coefficient to assess a linear association between the two variables. But we will also have a look at the Spearman’s for comparison. 

```{r}
cor.test( COPD$FEV1, COPD$MWT1Best, use="complete.obs", method = 'pearson')
```

```{r}
cor.test(COPD$FEV1, COPD$MWT1Best, use='complete.obs', method = 'spearman')
```


Pearson correlation = 0.4692142 
        95% confidence interval =  between 0.3 to 0.6
        p value is less than 0.001
       strong evidence against the null 
        
        
Spearman has similar results








and now with age and MWT1Best

```{r}
COPD %>% 
  select(MWT1Best) %>% 
  ggplot( aes(x= MWT1Best))+
  geom_histogram( bins = 10)+
  ggdark::dark_mode()
```


```{r}
COPD %>% 
  select(AGE) %>% 
  ggplot( aes(x= AGE))+
  geom_histogram( bins = 10)+
  ggdark::dark_mode()
```

```{r}
skimr::skim(COPD$AGE)

```


```{r}
COPD %>% 
  select(MWT1Best, AGE) %>% 
  ggplot( aes(y= MWT1Best, x= AGE))+
  geom_point()+
  ggdark::dark_mode()
```

predictor variable on the x-axis and outcome on the y-axis


Pearson's & Spearman's correlation
```{r}
cor.test(COPD$AGE, COPD$MWT1Best, use='complete.obs', method = 'pearson')

cor.test(COPD$AGE, COPD$MWT1Best, use='complete.obs', method = 'spearman')
```

Pearson's correlation = -0.23,    p-value = 0.0212   
Spearman's correlation = -0.269   p-value = 0.006

reject the null hypothesis




## Fitting a reression model

quantify the relationship between lung function (FEV1) and walking distance (MWT1Best) in COPD patients

linear regression : Y = $\alpha$ + $\beta_1$ * X + $\epsilon$

Y = outcome (i.e. dependent) variable 
X = predictor (i.e. independent) variable 

α and β (alpha & beta) are parameters of the regression
α = intercept (average Y when X=0), and 
β = slope of the line (change in Y for a 1 unit increase in X). 
ε (epsilon) is the random variation in Y, i.e. the residuals 

Note: α and β are unit specific, so you’ll get different answers if you use distance in metres and in feet. 


`lin_reg = lm( outcome ~ predictor, data= dataframe)`


linear regression to check whether lung function is a predictor of walking distance in COPD patients

```{r}
outcome = COPD$MWT1Best # walking distance
predictor = COPD$FEV1 # lung function

lin_reg = lm(outcome ~ predictor, data =  COPD)

summary( lin_reg)
```


to view the 9% confidence interval

```{r}
confint(lin_reg)
```
 
model plots

```{r}
plot(lin_reg)
```


predictor is the `FEV1` lung function = 74.11
meaning for 1 unit increase in `FEV1` it is predicted that walking distance will increase by 74 meters

95% Confidence Interval that the true population parameter is between 46 to 102 meters
adjusted r^2 = 0.21, so the regression model can explain only 21% of variability in observations

Q-Q plot, the data points land mostly on the horizontal line, meaning the assumptions of normality have been met

residuals vs fitted plot is to assess the assumptions of constant variance, there is some evidence based on the heteroscedasticity (unequal variance of 1 variable across another variable), but looking for homoscedasticity (equal variance of 1 variable across another variable). check for constant variance.





Age and walking distance in COPD patients

```{r}
walking_dist = COPD$MWT1Best
age = COPD$AGE

lin_reg2 = lm(walking_dist ~ age, data = COPD)

summary(lin_reg2)
confint(lin_reg2)
```

formula : alpha + beta * age
alpha = 616.45
beta = -3.10
r^2 = 0.0529


assumptions of a linear regression model are that: 

  (1) there is linearity between the outcome and predictor variable; 
  (2) that the outcome variable is normally distributed across values of the predictor; 
  (3) that the variance of the outcome variable is constant across values of the predictor variable. 
  
```{r}
predictedValues = predict(lin_reg2)
# predictedValues

residualValues = residuals(lin_reg2)
# residualValues

plot(lin_reg2)
```


residual: observed value - predicted value

normal Q-Q plot tells us if the residuals are normally distributed by comparing them with an actual normal distribution. focus on the center part where most of the data points should land, look at the points above or below the line at the tails. further away from line = more extreme the values in the data

scale location plot show if the residuals are spread equally among our predictions in order to check homoscedasticity 

residuals vs leverage plot shows influential data points that have a big effect on the linear model

```{r}
plot(residualValues)
```








## Multiple Regression

adjusted coefficients , the change in walking distance expected for 1 unit increase in FEV1 keeping age-health constant

if predictors were independent then th estimates in model model would be the same as univariate model


collinearity = when there is a strong linear association between 2 predictor variables in a model



quantify the relationship between the lung function (FEV1), age, and walking distance (MWT1Best) in COPD patients


multiple linear regression : Y = alpha + beta_1 * X_1 + beta_2 * X_2 + epsilon


Y = outcome (i.e. dependent) variable. 
X1 = first predictor (i.e. independent) variable. 
X2 = second predictor (i.e. independent) variable. 

alpha = intercept (average Y when X1=X2=0). Note: α is unit specific. 

B1 = slope of the line (change in Y for a 1 unit increase in X1 when X2 is held constant). Note: β1 is unit specific. 
Β2 = slope of the line (change in Y for a 1 unit increase in X2 when X1 is held constant). Note: β2 is unit specific. 

epsilon = is the random variation in Y, i.e. the residuals. 


`Model name <- lm( outcome ~ predictor1 + predictor2, data =dataframe)`


```{r}
walking_dist = COPD$MWT1Best # outcome var
age = COPD$AGE  # predictor 1
lung_function = COPD$FEV1 # predictor 2

mult_reg = lm(walking_dist ~ lung_function + age, data =  COPD)

summary(mult_reg)
confint(mult_reg)
```

`residuals`: the difference between the observed & predicted values
`residual mean` should be close to 0 (relative to data)
`min & max (1Q & 3Q)` should be close in magnitude, no huge range

`coefficient estimate` are used to predict the value of the response variable
`std error` is avg amount that the estimate varies from actual value
`t_value` = estimate / std. error
`r^2` is the % of the data variance that could be explained by the model
`adjusted r^2` controls for additional predictor added (prevent overfitting)
`F-stat` indicates if the model as a whole is statistically significant


intercept == alpha
lung function == beta_1
age = beta_2




























